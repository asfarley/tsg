# Multiple-Object Image Segmentation with VTC
---

## Introduction
VTC traffic-counting software has trouble distinguishing between single objects and several overlapping objects. To solve this problem, a neural network is trained to identify the number of objects in an image and estimate the locations and spatial information of the identified objects.

Training data can be real or synthetic. Real training data for this problem is expensive, so this document describes an approach for generating synthetic training data.

The generated data contains both simulated video of an intersection, and associated object information logs. The purpose of training the network is to produce object information logs using only access to the video stream.

## Software Prerequisites
Before proceeding, install the following packages:
 * VTC on Windows 10
 * Unity 5.6.1f1 on Windows 10
 * VMware Player on Windows 10
 * Ubuntu 16.04 LTS 64-bit on VMWare Player
 * Theano 0.9.0 on Ubuntu 16.04 LTS

## Simulated Video
Synthetic video footage is generated by running a simulation written in Unity (the video game engine). The simulation includes a 4-way intersection with vehicles performing turn maneuvers from different approaches. Right now, the simulation only includes one car model and a single camera vantage point. The simulation may need significant improvement in order to generate a wide enough variety of training footage.

#### How to generate synthetic video
Synthetic video is generated by launching the Unity application **TSG**. In the Hierarchy window, selection ConfigurationSingleton. In the Inspector window, the Root Output Path should be configurable. Select a good value for your own development environment (ex: C:\TSG). The Output Folder configuration item is appended to the Root Output Path to determine the output location for generated frames and object position data.

## File Formats
#### [*guid*] statehistory.txt
Generated by **TSG** in configured output path.

Contains the entire list of frames in which a particular object existed along with associated object information logs. One statehistory file exists for each object passing through the simulated field of view.

Example line:
```
-299.8 2.3 264.7 0 -2 0 -1823.2 436.3 -9825.5 103.8 2460.9 482.1 7/30/2017 12:17:01 PM 116
```

#### BoundingBoxInfo.txt
Generated by **VTC** when configuration option Export Training Data is enabled. Foreground blobs detected using background subtraction are saved to disk along with this file for future association with ground-truth object spatial information.

Contains a list of subframes exported using VTC for subitizing. This file is the first-stage output from VTC; the bounding boxes indicated in this file represent naive blobs which could contain zero or more objects. This file is processed using ground-truth object position logs (statehistory.txt files) to associate 'query' bounding boxes with 'response' bounding-box lists.

Example line:
```
Filename:"C:\VTCProject\vtc\bin\examples\MovingObjectSubimages\295.bmp" X:328.3 Y:381.5 Width:73 Height:39 Frame:1201
```

#### SpatialInfoContainedObjects.txt
Generated by ruby script **bblookup.rb** along with ground-truth files ([*guid*]statehistory.txt) and BoundingBoxInfo.txt

*e.x.* from Windows:
```
ruby bblookup.rb E:\TSG\Test\ C:\VTCProject\vtc\bin\examples\BoundingBoxInfo.txt C:\VTCProject\vtc\bin\examples\ClassifiedBoundingBoxInfo.txt
```

Contains a list of subframes exported using VTC for subitizing, along with the known quantity of objects in each subframe and a list of object spatial information associated with each object contained in this subframe.

This file is a 'comprehensive' metadata breakdown of an input subframe in terms of contained object quantity and spatial information. This file is targeted at human readers and downstream glue-logic parsing files used to generate the actual training set binary data.

Example line:
```
Filename:"C:\VTCProject\vtc\bin\examples\MovingObjectSubimages\58.bmp" X:897.0 Y:314.6 Width:67 Height:39 Frame:444 Count:2  [X:914.5 Y:300.4 Width:40.2 Height:32.6] [X:886.0 Y:319.7 Width:52.6 Height:42.6]
```

#### labels.txt
Generated by ruby script **generate_labels.rb** *e.x.*:
```
ruby generate_labels.rb boundingboxes.txt labels.txt
```


This file is the immediate precursor to dataset generation for Theano. This file contains annotated training set information (input image paths along with target one-hot output vectors). Only used for pure subitizing, does not contain spatial information.

Example line:
```
"C:\VTCProject\vtc\bin\examples\MovingObjectSubimages\13.bmp" 0 [1 0 0 0 0]
```

#### spatial.txt
Generated by: **TBD**

This file is the immediate precursor to dataset generation for Theano. This file contains annotated training set information (input image paths along with target spatial information lists). Used to train a network to output spatial information in list format using an input image.

Example line:
**TBD**

## Neural Network Architectures
VTC may require one or more neural-network architectures. The following variants are in development.

### Subitizing
Subitizing is the process of converting from an image or other simulus directly into a quantity response. The input is typically raw, continuous sensory data (e.g. image) and the output can be a single integer or a one-hot encoded vector.

Subitizing neural networks can use the same architecture as any other well-known classification architecture; The Convolutional MLP  from the Theano examples seems to work well. Any network capable of decent MNIST digit-classification performance is probably applicable to subitizing.

**Image &rArr; Quantity**

![Subitizing Illustration](https://dl.dropboxusercontent.com/u/59343634/Images/SubitizeIllustration.png "Subitizing vs Bounding-Box Set Output")

### Set Output
Standard neural networks output answers in the form of fixed-length vectors. This presents a problem for multiple object tracking; if we want a network to tell us the location of objects, we're asking for a variable-length list where order doesn't matter; in other words, we want our network to output a set rather than a vector. For the specific problem of identifying objects in a single image, we want the network to output a list of bounding-boxes containing objects relative to the input box coordinates.

The main difference in set-output neural network architectures is that the error function must be adjusted to account for the fact that a list of contained objects in an image may occur in any order without penalty. Thus, the output vector is permuted in a manner determined by the size of each element in the output list; the best match between permuted output vectors and the target output vector is selected.

**Image &rArr; [Position, Position, &hellip;]**

## Runtime Usage
In **VTC** in the module **VTC.Classifier.cs**, see the method for communication with the Theano ZeroMQ process:

```csharp
const string ClassifierEndpoint = "tcp://192.168.238.128:5555";
public static int[] ClassifyImagesZMQ(Image<Bgr, byte>[] images)
{
   if (images.Length == 0)
   {
       return new int[0];    
   }

   int[] classes = new int[images.Length];
   string[] classesLong = new string[images.Length];
   var args = new string[] { ClassifierEndpoint };
   string endpoint = args[0];

   // Create
   using (var context = new ZContext())
   using (var requester = new ZSocket(context, ZSocketType.REQ))
   {
       // Connect
       requester.Connect(endpoint);

       // Send
       var numBytes = 0;
       for (int k = 0; k < images.Length; k++)
           numBytes += images[k].Data.Length;

       var bytes = new byte[numBytes];
       var index = 0;
       for (int k = 0; k < images.Length; k++)
       for (int i = 0; i < images[k].Width; i++)
       for (int j = 0; j < images[k].Height; j++)
       {
           bytes[index++] = images[k].Data[j, i, 0];
           bytes[index++] = images[k].Data[j, i, 1];
           bytes[index++] = images[k].Data[j, i, 2];
       }

       requester.Send(new ZFrame(bytes));

       // Receive
       using (ZFrame reply = requester.ReceiveFrame())
       {
           var replyString = reply.ReadString();
           var filteredString = replyString.Replace("[", "").Replace("]", "");
           var classStrings = filteredString.Split(' ');
           classesLong = classStrings.Where(c => c != " ").ToArray();
       }
   }

   //Because Theano (and therefore ZeroMQ) returns a fixed-size array of classification responses
   for (int i = 0; i < images.Length; i++)
   {
       if (classesLong[i] == "0")
           classes[i] = 0;
       if (classesLong[i] == "1")
           classes[i] = 1;
       if (classesLong[i] == "2")
           classes[i] = 2;
       if (classesLong[i] == "3")
           classes[i] = 3;
   }

   return classes;
}
```

In **vtctheano** in the file **classify_images.py**, see the basic ZeroMQ message handling loop:
```python
# ZeroMQ Context
context = zmq.Context()

# Define the socket using the "Context"
sock = context.socket(zmq.REP)
sock.bind("tcp://192.168.238.128:5555")

# Run a simple "Echo" server
while True:
    message = sock.recv()

    print("Got ",sys.getsizeof(message), " bytes")
    message_bytes = bytearray(message)
    nImages = sys.getsizeof(message_bytes)/(28*28*3)
    if nImages > batch_size:
        nImages = batch_size

    print("Splitting into ", nImages, " images")

    imageArrays = []
    byte_start_index = 0
    byte_end_index = 28*28*3
    for imIndex in range(0,batch_size):
        if imIndex < nImages:
            #print("Parsing image ", imIndex)
            #print("start byte: ", byte_start_index, " end byte: ", byte_end_index)
            bytes_slice = message_bytes[byte_start_index:byte_end_index]
            bytes_index = 0
            im = numpy.empty([28,28,3])
            for i in range(0,28):
                for j in range (0,28):
                    im[j,i,0] = bytes_slice[bytes_index]
                    bytes_index += 1
                    #print("bytes_index is ", bytes_index)
                    im[j,i,1] = bytes_slice[bytes_index]
                    bytes_index += 1
                    #print("bytes_index is ", bytes_index, " j: ", j, " i: ", i)
                    im[j,i,2] = bytes_slice[bytes_index]
                    bytes_index += 1
                    #print("bytes_index is ", bytes_index)

            byte_start_index += 28*28*3
            byte_end_index += 28*28*3
            arr_f = numpy.divide(numpy.asarray(im, dtype="float32"), 255.0).transpose(2,0,1).flatten().astype(numpy.float64)
        imageArrays.append(arr_f)

    fake_validation_set_x = numpy.asarray([imageArrays[i] for i in range(batch_size)])
    valid_set_x = theano.shared(fake_validation_set_x)

    output = GetNetworkOutput(layer3, batch_size, index, x)
    print(numpy.array_str(output))
    sock.send(numpy.array_str(output))
```

At runtime, VTC extracts subframes and presents them to a trained network for classification. VTC transmits fixed-length lists of images to a ZeroMQ port running in Python inside an Ubuntu VM. Python decodes the byte payloads into batches of images which are presented to a Theano neural network. The neural network's response is submitted back to VTC over ZeroMQ.

Once each subframe has been subitized (and each contained object's spatial information has been estimated), the naive blob measurement list can be updated into a filtered object measurement list containing fewer false postives and better seperation of overlapping objects.
